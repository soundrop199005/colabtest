# -*- coding: utf-8 -*-
"""Level2_TrainModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YWML4ueaJF-v8cPXqbgHGhuq122ytTeX

# This is a simplified version of Level2 model training which wraps up the useful functions in Level2.ipynb with some examples
"""

#General imports
from __future__ import print_function
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import progressbar

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA
import gc
# Keras imports
import keras
import random
from keras.preprocessing.image import load_img
from keras.models import Sequential, Model
from keras.optimizers import *
from tensorflow.keras.utils import to_categorical
import keras.backend as K
import tensorflow as tf
# application (model) imports
from keras import applications
#from keras.applications.inception_v3 import preprocess_input
from keras.layers import Dense
import time
import math
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

"""Wrap up a function to import Cifar10 dataset, and split training and validation."""

# val_portion is the percentage of data you want to separate from the training set for validation purpose. Normally is 0.2 around.
# this function will return train images, train labels, validation images, validation labels in numpy arrays. All the labels are one hot encoded.

def prepare_train_val_data(val_portion):
  # downloading cifar data
  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
  assert x_train.shape == (50000, 32, 32, 3)
  assert x_test.shape == (10000, 32, 32, 3)
  assert y_train.shape == (50000, 1)
  assert y_test.shape == (10000, 1)
  # one hot encoding
  print("First 5 training labels: ", y_train[:5])
  y_train = to_categorical(y_train, 10)
  y_test = to_categorical(y_test, 10)
  print("First 5 training lables as one-hot encoded vectors:\n", y_train[:5])
  print(y_train.shape)
  print(y_test.shape)
  trainX, valX, trainY, valY = train_test_split(x_train, y_train, test_size=val_portion, random_state=42, shuffle = True, stratify=y_train)
  [print(each.shape) for each in [trainX, trainY, valX, valY]]
  return trainX, trainY, valX, valY

# sample to have a look

def sample_show(x,y):
  # random generate a number from 0 to length-1
  index_sa = random.randint(0, len(y)-1)
  print(index_sa)
  plt.imshow(x[index_sa])
  print(y[index_sa])

"""Let's define a function to construct a new model. This new model removes the last 2 bottleneck blocks from the MobileNetV2 and output 10 classes in the final dense layer. I want hyperparameter
tuning the fully connected layer input units - original 1280 with the dropout. Also I want to add the weight decay.
"""

# alp is the alpha - width multiplier in MobileNetV2 model architecture
# fc_units is the input size for the last fully connected layer
# fc_dropout is the dropout rate for the input units of the last fully connected layer
# weight_decay is the L2 norm alpha

# This function will output the model strucutre you customized and print the model summary out.

def new_model(alp,fc_units,fc_dropout,weight_decay) -> keras.Sequential:
  inputs = keras.Input(shape=(32, 32, 3))
  preprocess_input = applications.mobilenet_v2.preprocess_input
  x = preprocess_input(inputs)
  base_model_2 = applications.MobileNetV2(
  input_shape=(32,32,3),
  alpha=alp,
  include_top=True,
  weights= None,
  input_tensor=None,
  pooling=None,
  classes=10,
  classifier_activation="softmax")
  feature_extractor = keras.Model(base_model_2.input, base_model_2.layers[115].output)
  x_h = feature_extractor(x, training=True)
  x_c = keras.layers.Conv2D(filters = fc_units, kernel_size = 1, input_shape = x_h.shape[1:])(x_h)
  x_b = tf.keras.layers.BatchNormalization()(x_c)
  x_r = tf.keras.layers.ReLU()(x_b)
  x_a = tf.keras.layers.GlobalAveragePooling2D()(x_r)
  x_d = tf.keras.layers.Dropout(fc_dropout)(x_a)
  outputs = tf.keras.layers.Dense(10,activation='softmax')(x_d)
  model = tf.keras.Model(inputs, outputs)
  alpha = weight_decay  # weight decay coefficient
  for layer in model.layers:
      if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense):
          layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.kernel))
      if hasattr(layer, 'bias_regularizer') and layer.use_bias:
          layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.bias))
  print(model.summary())

  return model

"""This function is to plot the model training process"""

# history is the model training history retrieved
# filename is what you want to save the traing process graph as, which should be a valid directory

def plot_loss_accuracy_s(history,filename):
    historydf = pd.DataFrame(history.history, index=history.epoch)
    plt.figure(figsize=(8, 6))
    historydf.plot(ylim=(0, max(1, historydf.values.max())))
    loss = min(history.history['val_loss'])
    mylist1 = history.history['val_loss']
    ind = mylist1.index(min(mylist1))
    acc =history.history['val_accuracy'][ind]
    plt.title('Val Loss: %.3f, Val Accuracy: %.3f' % (loss, acc))
    plt.savefig(filename)

"""Define a function for hyper parameter tuning more easily"""

# alp is the alpha - width multiplier in MobileNetV2 model architecture, around 1.
# lr is learning rate for model training
# m is momentum for model training
# wd is the L2 norm alpha for weight decay
# fc_u is the input size (the number of input units) for the last fully connected layer
# fc_d is the dropout rate for the input units of the last fully connected layer
# n_epoch is the maximum number of epochs you want to run. The model might be trained less epochs due to early stopping.
# directory is where you want to save your model to
# train_X is the training image numpy or tensor
# train_Y is the training labels in one hot encoder
# val_X is the validation image numpy or tensor
# val_Y is the validation labels in one hot encoder

# This function will train the model with your hyper parameter setting and save the best model during training into your folder.
# The model will be trained with your hyper parameter setting as listed above. Batch size is fixed to 64. Optimizer is RMSprop.
# It is a multiclassification problem. The last layer is activated by softmax. I used categorical cross entropy loss.
# During the training, the validation loss will be monitored. If the validation loss does not drop after 15 iterations, the model training stops automatically unless it reaches the maximum epochs earlier.
# The training process is saved into a graph. Under the folder you specified, you will see a png file called something like new_modelV2_alpha_%.3f_lr_%.9f_m_%.2f_wd_%.7f_fcu_%.1f_fcd_%.2f.png
# Only the best model during the training will be saved into a subfolder in h5 format. The sub folder name is something like new_modelV2_alpha_%.3f_lr_%.9f_m_%.2f_wd_%.7f_fcu_%.1f_fcd_%.2f
# This function returns the best model validation loss which you can use to compare among different models later for hyper parameter tuning purpose.

def hyp_tuning(alp, lr, m, wd,fc_u,fc_d,n_epoch, directory,train_X, train_Y,val_X,val_Y):
  # construct model
  model = new_model(alp,fc_u,fc_d,wd)
  # learning rate schedule
  lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=lr,
    decay_steps=50,
    decay_rate=0.98)
  # optimizer
  opt = tf.keras.optimizers.experimental.RMSprop(
      learning_rate=lr_schedule,
      rho=0.9,
      momentum=m,
      epsilon=1e-07,
      centered=False,
      # we used weight decay in the model by L2 norm instead
      weight_decay=0,
      clipnorm=None,
      clipvalue=None,
      global_clipnorm=None,
      use_ema=False,
      ema_momentum=0.99,
      ema_overwrite_frequency=100,
      jit_compile=True,
      name='RMSprop'
  )
  # callbacks
  parent_dir = os.path.join(directory, 'new_modelV2_alpha_%.3f_lr_%.9f_m_%.2f_wd_%.7f_fcu_%.1f_fcd_%.2f'%(alp,lr,m,wd,fc_u,fc_d))
  if not os.path.exists(parent_dir):
    os.mkdir(parent_dir)
  checkpoint_path = os.path.join(parent_dir, 'model.h5')
  print(checkpoint_path)
  my_callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=0, mode='min'),
    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min')]
  # compile model
  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
  # fit model
  history = model.fit(train_X, train_Y, validation_data=(val_X,val_Y), epochs=n_epoch, verbose=True, batch_size=64, shuffle=True,callbacks= my_callbacks)
  plot_loss_accuracy_s(history,parent_dir+'.png')
  return min(history.history['val_loss'])

"""A function to train the model with the best hyper parameter setting found so far from hyper parameter tuning"""

# dir is where you want to save your model to
# train_X is the training image numpy or tensor
# train_Y is the training labels in one hot encoder
# val_X is the validation image numpy or tensor
# val_Y is the validation labels in one hot encoder

# This function will train the model with the best hyper parameter setting found so far.
# It used early stopping with 50 maximum epochs.
# The training process graph will be saved into your directory.
# Only the best model during the training is saved into a sub folder in your defined folder.

def train_best(dir,trainX, trainY, valX, valY):
  alp = 0.9
  lr = 0.0001
  m = 0.9
  wd = 1e-05
  n_e = 50
  fc_u = 1000
  fc_d = 0.2
  hyp_tuning(alp, lr, m, wd, fc_u, fc_d, n_e, dir,trainX, trainY,valX,valY)

"""Let's try all these functions.
Firstly input data
"""

trainX, trainY, valX, valY = prepare_train_val_data(0.2)

"""Have a look"""

sample_show(trainX, trainY)

sample_show(valX, valY )

"""Build your customized models.  You can experiment more for hyper parameter tuning purpose.

Train the model with the best hyper parameter setting found so far and save it into your current working directory.
"""

cwd = os. getcwd()
print(cwd)

train_best(cwd, trainX, trainY, valX, valY)